{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6238ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ef75015",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('train.csv')\n",
    "data_test = pd.read_csv('test.csv')\n",
    "X = data_train.text\n",
    "y = data_train.rating\n",
    "X_final_test = data_test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78520e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para limpieza de texto\n",
    "import re\n",
    "\n",
    "def limpiar(list):\n",
    "    my_valoraciones_new = []\n",
    "    for x in list:\n",
    "        if type(x) == str:\n",
    "            x = x.lower()\n",
    "            x = re.sub(r\"[n]*$\", \" \", x)\n",
    "            x = re.sub(r\"that's\",\"that is\",x)\n",
    "            x = re.sub(r\"there's\",\"there is\",x)\n",
    "            x = re.sub(r\"what's\",\"what is\",x)\n",
    "            x = re.sub(r\"where's\",\"where is\",x)\n",
    "            x = re.sub(r\"it's\",\"it is\",x)\n",
    "            x = re.sub(r\"who's\",\"who is\",x)\n",
    "            x = re.sub(r\"i'm\",\"i am\",x)\n",
    "            x = re.sub(r\"she's\",\"she is\",x)\n",
    "            x = re.sub(r\"he's\",\"he is\",x)\n",
    "            x = re.sub(r\"they're\",\"they are\",x)\n",
    "            x = re.sub(r\"who're\",\"who are\",x)\n",
    "            x = re.sub(r\"ain't\",\"am not\",x)\n",
    "            x = re.sub(r\"wouldn't\",\"would not\",x)\n",
    "            x = re.sub(r\"shouldn't\",\"should not\",x)\n",
    "            x = re.sub(r\"can't\",\"can not\",x)\n",
    "            x = re.sub(r\"couldn't\",\"could not\",x)\n",
    "            x = re.sub(r\"won't\",\"will not\",x)\n",
    "            x = re.sub(r\"\\W\",\" \",x)\n",
    "            x = re.sub(r\"\\d\",\" \",x)\n",
    "            x = re.sub(r\"\\s+[a-z]\\s+\",\" \",x)\n",
    "            x = re.sub(r\"\\s+[a-z]$\",\" \",x)\n",
    "            x = re.sub(r\"^[a-z]\\s+\",\" \",x)\n",
    "            x = re.sub(r\"\\s+\",\" \",x)\n",
    "            my_valoraciones_new.append(x)\n",
    "    return my_valoraciones_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f8f1093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpio la X train y la X test con re\n",
    "X_limpio = limpiar(X)\n",
    "X_final_test_limpio = limpiar(X_final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08729bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizar los datos de texto utilizando CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X_limpio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e57d3cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizo el X train\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "'''\n",
    "TF (Term Frequency - Frecuencia de Término): Mide la importancia de una palabra en un documento específico. \n",
    "Cuantas más veces aparece una palabra en un documento, mayor es su peso TF.\n",
    "vectorizer = TfidfVectorizer(max_features = 50)\n",
    "IDF (Inverse Document Frequency - Frecuencia Inversa de Documento): Mide la importancia de una palabra en el contexto \n",
    "de un conjunto de documentos. Cuanto menos común es una palabra en todos los documentos, mayor es su peso IDF\n",
    "TF-IDF Score: El puntaje TF-IDF de una palabra en un documento se obtiene multiplicando su valor TF por su valor IDF. \n",
    "Esto da como resultado un puntaje que refleja la importancia de la palabra tanto en el documento \n",
    "como en el conjunto de documentos. La fórmula general para el puntaje TF-IDF de una palabra en un documento es:\n",
    "TF-IDF(t,d,D)=TF(t,d)×IDF(t,D)TF-IDF(t,d,D)=TF(t,d)×IDF(t,D)\n",
    "Vectorización: Después de calcular los puntajes TF-IDF para todas las palabras en un conjunto de documentos, \n",
    "cada documento se representa como un vector numérico donde cada dimensión del vector corresponde a una palabra \n",
    "y el valor en esa dimensión es el puntaje TF-IDF de esa palabra en el documento. \n",
    "Esto crea una representación vectorial densa de los documentos que se puede utilizar en algoritmos de aprendizaje automático.\n",
    "\n",
    "En resumen, el TF-IDF Vectorizer transforma documentos de texto en vectores numéricos ponderados que capturan \n",
    "la importancia relativa de las palabras en cada documento en el contexto de un conjunto de documentos. \n",
    "Esto es útil para tareas como [[[recuperación de información, clasificación de texto y agrupación de documentos]]], \n",
    "ya que permite a los algoritmos de aprendizaje automático trabajar con datos de texto de manera efectiva.\n",
    "\n",
    "PARAMETROS: \n",
    "max_features: máximo numero de palabras a obtener\n",
    "min_df/max_df : frecuencia de aparición máxima o mínima en los documentos\n",
    "stop_words: palabras a eliminar\n",
    "binary (default=False): Si True,resultado 1 o 0, indicando la presencia o ausencia de una palabra en un documento.\n",
    "\n",
    "'''\n",
    "vectorizer = TfidfVectorizer(max_features= 1000) # max_features, min/max_df, stop_words, binary(False)\n",
    "\n",
    "X = vectorizer.fit(X_limpio,X_final_test_limpio)\n",
    "#Vectorizo la X\n",
    "X_final_test = vectorizer.transform(X_limpio).toarray()\n",
    "X_final_test = vectorizer.transform(X_final_test_limpio).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e5a35dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divido el dataset en entrenamiento y validación\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76924d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6716723549488055\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento con random forest:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "'''\n",
    "Crea arboles con columnas y filas aleatorias del dataset y va dividiendo en ramas hasta que llega al final\n",
    "n_estimators[d=100]: Numero de arboles(aleatorios) en el bosque\n",
    "max_depth[d=None, hasta min_samples_split]: Profundidad máxima de los árboles.\n",
    "min_samples_split[d=2]: Número mínimo de muestras requeridas para dividir un nodo interno.\n",
    "min_samples_leaf[d=1]: Número mínimo de muestras requeridas para ser una hoja\n",
    "max_features[d=raiz cuadrada total caracteristicas]: Número de características a considerar para encontrar la mejor división en cada árbol\n",
    "\n",
    "'''\n",
    "random_forest_classifier = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "random_forest_classifier.fit(X_train, y_train)\n",
    "y_pred_forest = random_forest_classifier.predict(X_test)\n",
    "print(sklearn.metrics.f1_score(y_test, y_pred_forest, average='micro'))\n",
    "y_final_randomforest = random_forest_classifier.predict(X_final_test)\n",
    "#Con 1000 palabras 0,68, 2000: 0.67, 500: 0,678 , 50 = 0.670\n",
    "#1000 palabras: 50 estimators:0,674,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffdffd9",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Clasificación y análisis de texto, especialmente NPL. \n",
    "Funciona en base al teorema de Bayes, que es una regla de probabilidad condicional, \n",
    "y asume independencia condicional entre las características, lo que le da el nombre de \"Naive\"\n",
    "calcular la probabilidad de una hipótesis (clase) basada en la evidencia observada (características)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2bffa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2887372013651877\n"
     ]
    }
   ],
   "source": [
    "# SOLO PARA DATOS CONTINUOS\n",
    "# el clasificador Naive Bayes Gaussiano asume que las características son continuas y siguen una distribución gaussiana\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "naive_gaussian = GaussianNB()\n",
    "naive_gaussian.fit(X_train, y_train)\n",
    "y_pred_naive_g = naive_gaussian.predict(X_test)\n",
    "print(sklearn.metrics.f1_score(y_test, y_pred_naive_g, average='micro'))\n",
    "y_final_naive_g = naive_gaussian.predict(X_final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3ab5da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6935153583617747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amand\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\amand\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento con MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#alpha[d=1] = suaviza los datos para evitar la probabilidad nula \n",
    "naive_bayes_classifier = MultinomialNB(alpha=0)\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "y_pred_naive_m = naive_bayes_classifier.predict(X_test)\n",
    "print(sklearn.metrics.f1_score(y_test, y_pred_naive_m, average='micro'))\n",
    "y_final_naive_m = naive_bayes_classifier.predict(X_final_test)\n",
    "#Con 1000 palabras 0,679, 2000: 0,670, 500:0.677, 50; 0,66\n",
    "# Con alpha 0 : 0.693\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "988a71df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6450511945392492\n"
     ]
    }
   ],
   "source": [
    "#Entrenamiento con BernoulliNB\n",
    "# [[[BINARIO]]]\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bernoulli = BernoulliNB()\n",
    "bernoulli.fit(X_train, y_train)\n",
    "y_pred_naive_b = bernoulli.predict(X_test)\n",
    "print(sklearn.metrics.f1_score(y_test, y_pred_naive_b, average='micro'))\n",
    "y_final_naive_b = bernoulli.predict(X_final_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe1cab28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7044368600682593\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento con Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# En Regresión Logística, se utiliza la función logística (también llamada función sigmoide) \n",
    "# para modelar la probabilidad de que una instancia pertenezca a una clase particular.\n",
    "# La función sigmoide transforma una combinación lineal de las características en un valor en el rango de 0 a 1.\n",
    "# Para usarlo para mas de 2 clases se usa multi_class='multinomial', solver='lbfgs'\n",
    "LR = LogisticRegression(multi_class='multinomial')\n",
    "LR.fit(X_train, y_train)\n",
    "y_pred_regres = LR.predict(X_test)\n",
    "print(sklearn.metrics.f1_score(y_test, y_pred_regres, average='micro'))\n",
    "y_final_regres = LR.predict(X_final_test)\n",
    "# 1000:0.698, 2000: 0,698, 500: 0.6969, 50: 0,679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a260c88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6505119453924915\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento con Knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Para clasificacion y regresion\n",
    "# weights='unifor,', 'distance' Peso de los vecinos(uniforme o por distancia)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "print(sklearn.metrics.f1_score(y_test, y_pred_knn, average='micro'))\n",
    "y_final_knn = knn.predict(X_final_test)\n",
    "# 1000:0.414, 2000: 0.652, 500: 0.640, 50: 0,645\n",
    "#nn=3: 0'65,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c58911a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6027303754266211\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento con arboles de decision\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "'''\n",
    "    criterion: El criterio utilizado para medir la calidad de una división. \n",
    "    Puede ser 'gini' para el índice Gini o 'entropy' para la ganancia de información. \n",
    "    El índice Gini es el valor por defecto.\n",
    "\n",
    "    max_depth: La profundidad máxima del árbol. Puedes establecer un valor entero para limitar \n",
    "    la profundidad del árbol. Si se establece en None, el árbol se expandirá hasta que todas las hojas sean puras \n",
    "    o hasta que contengan menos de min_samples_split muestras.\n",
    "\n",
    "    min_samples_split: El número mínimo de muestras requeridas para dividir un nodo interno. \n",
    "    Puede ser un valor entero o un número decimal.\n",
    "\n",
    "    min_samples_leaf: El número mínimo de muestras requeridas en una hoja. \n",
    "    Puede ser un valor entero o un número decimal.\n",
    "\n",
    "    max_features: El número máximo de características que se consideran al hacer una división. \n",
    "    Puede ser un valor entero o un número decimal.\n",
    "'''\n",
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=None, min_samples_split=2, min_samples_leaf=1)\n",
    "clf = clf.fit(X_train,y_train)\n",
    "y_pred_tree = clf.predict(X_test)\n",
    "print(sklearn.metrics.f1_score(y_test, y_pred_tree, average='micro'))\n",
    "y_final_tree = clf.predict(X_final_test)\n",
    "# 1000: 0.666, 2000: 0.658, 500: 0.662, xdefect= 0.662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3e1101f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5384</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3529</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3640</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1263</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7483</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>3734</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>4518</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>1358</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>4348</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>1443</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1954 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  rating\n",
       "0     5384       5\n",
       "1     3529       5\n",
       "2     3640       5\n",
       "3     1263       5\n",
       "4     7483       5\n",
       "...    ...     ...\n",
       "1949  3734       5\n",
       "1950  4518       5\n",
       "1951  1358       5\n",
       "1952  4348       5\n",
       "1953  1443       5\n",
       "\n",
       "[1954 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RESULTADO\n",
    "resul = pd.DataFrame()\n",
    "resul['id']= data_test.id\n",
    "resul['rating'] = y_final_regres\n",
    "resul.to_csv('resultado_c.csv', index=False)\n",
    "resul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623f256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
